{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b600721f-ae48-45dd-9ebd-6726c84a434f",
   "metadata": {},
   "source": [
    "# RandomForestClassifier\n",
    "The RandomForestClassifier in sklearn.ensemble is a versatile and powerful classifier based on an ensemble of decision trees. It works by training multiple decision trees on random subsets of the data and averaging their predictions to improve accuracy and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48308a1-abe1-46fb-8b7c-5442cb422d5e",
   "metadata": {},
   "source": [
    "class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5872c5c4-c1fc-4c9d-8b56-464db0969267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c85576-0b9c-41f4-8f20-b5d747090814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Forest_Classifier:\n",
    "    def __init__(self,n_trees=10,max_depth=10,min_samples_split=2):\n",
    "        self.n_trees=n_trees\n",
    "        self.max_depth=max_depth\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.n_feature=None\n",
    "        self.trees=[]\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        for i in range(self.n_trees):\n",
    "            tree=DecisionTreeClassifier(max_depth=self.max_depth,min_samples_split=self.min_samples_split)\n",
    "            X_sample,y_sample=self._bootstrap_samples(X,y)\n",
    "            tree.fit(X_sample,y_sample)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def _bootstrap_samples(self,X,y):\n",
    "        n_samples=X.shape[0]\n",
    "        indx=np.random.choice(n_samples,n_samples,replace=True)\n",
    "        return X[indx],y[indx]\n",
    "\n",
    "    def _most_common_label(self,y):\n",
    "        counter=Counter(y)\n",
    "        most_common=counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "    \n",
    "    def predict(self,X):\n",
    "        predictions=np.array([tree.predict(X) for tree in self.trees])\n",
    "        pred=np.swapaxes(predictions,0,1)\n",
    "        return np.array([self._most_common_label(p) for p in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1611cba7-6bfe-48a1-9aa3-ed2dab5e1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b498074c-5bb4-4f1a-9a19-c1db92f5e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"diabetes.csv\")\n",
    "X=df.drop(columns=[\"Outcome\"])\n",
    "y=df[\"Outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "059e27ba-5f7f-4074-ad56-8ce49b8d3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defc5235-b731-420a-a3e0-341fe216f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "x_test=np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "720b4190-1a69-496c-879a-74e5988962ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf=Random_Forest_Classifier()\n",
    "rf.fit(X_train,y_train)\n",
    "predictions=rf.predict(np.array(X_test))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c9b3fb-8e7a-4042-8041-51fd28f2fb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc8810b-1925-4196-ad7f-ef5bc23303ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true,y_pred):\n",
    "    accuracy=np.sum(y_true==y_pred)/len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0de75c2-75b8-4738-9a12-fd3cf1295dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7727272727272727"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b47e4e0-f4ff-4007-a6e2-ac02a8aea1e9",
   "metadata": {},
   "source": [
    "### Key Parameters:\n",
    "- n_estimators: The number of trees in the forest (default: 100).\n",
    "- criterion: Function to measure the quality of a split (\"gini\", \"entropy\", \"log_loss\").\n",
    "- max_depth: Maximum depth of the trees (default: None, meaning trees grow until leaves are pure or below a threshold).\n",
    "- min_samples_split: Minimum number of samples required to split a node.\n",
    "- min_samples_leaf: Minimum number of samples at a leaf node.\n",
    "- max_features: Number of features to consider when looking for the best split (default: 'sqrt').\n",
    "- bootstrap: Whether to use bootstrap samples to build trees (default: True).\n",
    "- oob_score: Whether to use out-of-bag samples to estimate accuracy (default: False).\n",
    "- n_jobs: The number of jobs to run in parallel (-1 for all cores).\n",
    "- random_state: For reproducibility.\n",
    "- class_weight: Weights associated with classes for handling imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "117e5257-3ec9-495e-b2eb-6bfafb6d9461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gyanb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10,max_depth=10,min_samples_split=2)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions=clf.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7be73-ea42-4932-b9e0-da10e8e8d433",
   "metadata": {},
   "source": [
    "### Important Attributes:\n",
    "- estimators_: Collection of fitted trees.\n",
    "- feature_importances_: Importance of each feature based on the reduction in impurity.\n",
    "- oob_score_: The out-of-bag score, if oob_score=True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fad5be-152d-40b9-99cf-6b58e032e1b7",
   "metadata": {},
   "source": [
    "### Key Methods:\n",
    "- fit(X, y): Fits the model to the data.\n",
    "- predict(X): Predicts classes for X.\n",
    "- apply(X): Returns leaf indices for X.\n",
    "- decision_path(X): Returns the decision path in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43601124-47e5-4c49-989f-ad7c3056754f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "085b2663-b063-4d7e-9b2f-729c8d5e83cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7402597402597403"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c4aaf-1acc-482f-9add-3eedb080ac3a",
   "metadata": {},
   "source": [
    "### Example: python\n",
    "\n",
    "- from sklearn.ensemble import RandomForestClassifier<br>\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "- Generate a random dataset<br>\n",
    "X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, random_state=0)\n",
    "\n",
    "- Create and train the model<br>\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)<br>\n",
    "clf.fit(X, y)\n",
    "\n",
    "- Make predictions<br>\n",
    "predictions = clf.predict([[0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad9f66-138f-429a-97db-345533f3ae04",
   "metadata": {},
   "source": [
    "### Use Cases:\n",
    "- Effective in classification tasks.\n",
    "- Handles large datasets and high-dimensional spaces well.\n",
    "- Robust to overfitting with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d1cd5e-c88f-4a9f-8768-8a4f23ef1fd7",
   "metadata": {},
   "source": [
    "# RandomForestRegressor\n",
    "The RandomForestRegressor in scikit-learn is a powerful and versatile machine learning model used for regression tasks. It leverages the ensemble learning technique by constructing multiple decision trees during training and averaging their predictions to improve accuracy and control overfitting.\n",
    "\n",
    "### Overview:\n",
    "- Random Forest is an ensemble method that builds a collection (forest) of decision tree regressors.\n",
    "- Each tree is trained on a random subset of the data (bootstrap sampling) and a random subset of features for splitting nodes.\n",
    "- The final prediction is the average of the predictions from all individual trees, which enhances the model's robustness and generalization capabilities compared to a single decision tree.\n",
    "\n",
    "class sklearn.ensemble.RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4536de82-0e67-4ef6-a5de-5830fffb9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Forest_Regressor:\n",
    "    def __init__(self,n_trees=10,max_depth=10,min_samples_split=2):\n",
    "        self.n_trees=n_trees\n",
    "        self.max_depth=max_depth\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.n_feature=None\n",
    "        self.trees=[]\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        for i in range(self.n_trees):\n",
    "            tree=DecisionTreeRegressor(max_depth=self.max_depth,min_samples_split=self.min_samples_split)\n",
    "            X_sample,y_sample=self._bootstrap_samples(X,y)\n",
    "            tree.fit(X_sample,y_sample)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def _bootstrap_samples(self,X,y):\n",
    "        n_samples=X.shape[0]\n",
    "        indx=np.random.choice(n_samples,n_samples,replace=True)\n",
    "        return X[indx],y[indx]\n",
    "\n",
    "    \n",
    "    def predict(self,X):\n",
    "        predictions=np.array([tree.predict(X) for tree in self.trees])\n",
    "        pred=np.swapaxes(predictions,0,1)\n",
    "        return np.array([np.mean(p) for p in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ca11945-75ea-4dfc-bef3-ea437a9d72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Boston.csv\")\n",
    "X=df.drop(columns=[\"medv\"])\n",
    "y=df[\"medv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4baab76d-05f2-4ae9-b5ec-cbd39819316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "955d2972-a2e9-417a-942c-e7ac30510762",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "x_test=np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8ccc989-c73d-4cd8-9e83-bfd38dd6b6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31.545     , 26.62      ,  7.51685714, 21.81761381, 13.46      ,\n",
       "       23.72061709, 19.90275253, 16.98280556, 20.29096032, 29.63041667,\n",
       "       16.69      , 21.27779453, 21.18760546, 18.18115333, 19.9172514 ,\n",
       "       21.9425    ,  9.75779798, 20.19464539, 13.30725   ,  7.51685714,\n",
       "       35.46111111, 19.96033333, 19.81976492, 16.59714286, 22.82806164,\n",
       "       20.44255943, 20.83546129, 23.59818056, 29.29      , 19.52719697,\n",
       "       20.70055983, 16.80512158, 19.60504717, 13.32      , 24.028     ,\n",
       "       16.5       , 20.30346032, 22.11405884,  9.31285714,  7.59622222,\n",
       "        6.96066667, 23.23      , 47.66      , 27.74      , 15.51475   ,\n",
       "       24.98413889, 26.86780303, 45.97      , 16.591875  , 20.43363636,\n",
       "       20.65519339, 32.545     , 27.10720836, 23.63763086, 37.75      ,\n",
       "       32.17      ,  9.30467532, 32.545     , 30.1       , 21.42790939,\n",
       "       35.71771825, 17.16183333, 24.95054711, 37.39111111,  9.51848485,\n",
       "       21.15374315, 33.5       , 19.66907138, 49.28      , 45.21      ,\n",
       "       16.41717949, 23.85083117, 15.44      , 19.4330303 , 23.39834514,\n",
       "       14.22090909, 35.08      , 20.85975   , 16.87719697, 16.28636364,\n",
       "       44.31      , 41.865     , 24.59115397, 16.47056458, 26.05144444,\n",
       "       12.24174603, 26.79097222, 36.13      , 20.36219818, 21.96305857,\n",
       "       17.08547872, 14.34555556, 22.13123862, 18.26065657, 15.75333333,\n",
       "       45.43      , 15.7       , 22.61829545, 30.06833333, 28.06205303,\n",
       "       18.67140585, 23.18264603])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf=Random_Forest_Regressor()\n",
    "rf.fit(X_train,y_train)\n",
    "predictions=rf.predict(np.array(X_test))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb92f44a-e62e-4b1e-b9f8-3db3f214f319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33. , 27.5,  5.6, 21.2, 14.9, 22.3, 18.8, 14.6, 19.4, 32. , 13.8,\n",
       "       21.7, 22.6, 18.4, 20.5, 22.2, 10.8, 22.5, 13.8,  5. , 32.9, 18.6,\n",
       "       16.8, 27.1, 22.9, 19.6, 22.7, 28.1, 26.6, 18.8, 18.9, 14.8, 19.6,\n",
       "       17.1, 22.6, 27.5, 22.8, 21. ,  7.4, 10.4,  8.5, 21. , 45.4, 28.2,\n",
       "       14.2, 22. , 24.1, 43.5, 15.2, 22. , 20.8, 36.1, 27. , 23.4, 50. ,\n",
       "       31.6,  5. , 32.7, 30.1, 23. , 34.9, 14.1, 22.8, 36.4,  8.3, 22. ,\n",
       "       36.5, 16.1, 50. , 44.8, 13.1, 23.1, 12. , 21.5, 24.2, 13.1, 50. ,\n",
       "       20.6, 17.3, 13.3, 37.6, 48.8, 24.1, 14.3, 36.2, 11.9, 27.1, 29.6,\n",
       "       20.4, 21.2, 13.9, 13.6, 22. , 19.7, 15.4, 46.7, 11.7, 22.9, 30.5,\n",
       "       23.3, 19.4, 23. ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "314a4471-a5a1-4a55-a197-e501dbc87c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true,y_pred):\n",
    "    mse=np.sum((y_true-y_pred)**2)/len(y_true)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfa2568a-5a73-40f6-8b7a-9089c583b127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.742058617355111"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3872ee5-0d2e-42dc-85a4-5502e7b8dd53",
   "metadata": {},
   "source": [
    "### Key Parameters:\n",
    "- n_estimators (int, default=100):\n",
    "\n",
    "Description: Number of trees in the forest.\n",
    "Note: Increased number can improve performance but also increase computational cost.\n",
    "- criterion ({\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"}, default=\"squared_error\"):\n",
    "\n",
    "Description: Function to measure the quality of a split.\n",
    "squared_error: Mean Squared Error (MSE).\n",
    "absolute_error: Mean Absolute Error (MAE).\n",
    "friedman_mse: MSE with Friedman's improvement score.\n",
    "poisson: Reduction in Poisson deviance.\n",
    "Note: \"absolute_error\" can be significantly slower than \"squared_error\".\n",
    "- max_depth (int, default=None):\n",
    "\n",
    "Description: Maximum depth of the trees.\n",
    "Behavior: If None, trees expand until all leaves are pure or contain fewer than min_samples_split samples.\n",
    "- min_samples_split (int or float, default=2):\n",
    "\n",
    "Description: Minimum number of samples required to split an internal node.\n",
    "Behavior:\n",
    "If int, it's the exact number.\n",
    "If float, it's a fraction of the total samples.\n",
    "- min_samples_leaf (int or float, default=1):\n",
    "\n",
    "Description: Minimum number of samples required to be at a leaf node.\n",
    "Behavior:\n",
    "If int, it's the exact number.\n",
    "If float, it's a fraction of the total samples.\n",
    "Effect: Helps in smoothing the model, especially useful in regression.\n",
    "- max_features ({\"sqrt\", \"log2\", None}, int, or float, default=1.0):\n",
    "\n",
    "Description: Number of features to consider when looking for the best split.\n",
    "Options:\n",
    "\"sqrt\": Square root of the number of features.\n",
    "\"log2\": Logarithm base 2 of the number of features.\n",
    "int: Exact number of features.\n",
    "float: Fraction of total features.\n",
    "None or 1.0: All features.\n",
    "Note: Smaller values increase randomness and reduce correlation among trees.\n",
    "- bootstrap (bool, default=True):\n",
    "\n",
    "Description: Whether to use bootstrap samples (sampling with replacement) when building trees.\n",
    "Effect: False means the whole dataset is used to build each tree.\n",
    "- oob_score (bool or callable, default=False):\n",
    "\n",
    "Description: Whether to use out-of-bag samples to estimate the generalization score.\n",
    "Usage: Only available if bootstrap=True.\n",
    "Default Metric: r2_score, but can provide a custom metric via a callable.\n",
    "- random_state (int, RandomState instance, or None, default=None):\n",
    "\n",
    "Description: Controls the randomness of the bootstrapping and feature sampling.\n",
    "Usage: Ensures reproducibility of results.\n",
    "- n_jobs (int, default=None):\n",
    "\n",
    "Description: Number of jobs to run in parallel.\n",
    "Options:\n",
    "-1: Use all available cores.\n",
    "None: Defaults to 1 unless within a joblib.parallel_backend context.\n",
    "- ccp_alpha (float, default=0.0):\n",
    "\n",
    "Description: Complexity parameter for Minimal Cost-Complexity Pruning.\n",
    "Effect: Controls tree size by pruning nodes with the least contribution to impurity reduction.\n",
    "- max_samples (int or float, default=None):\n",
    "\n",
    "Description: If bootstrap=True, the number of samples to draw from X to train each base estimator.\n",
    "Behavior:\n",
    "None: Draw all samples (X.shape[0]).\n",
    "int: Exact number of samples.\n",
    "float: Fraction of total samples.\n",
    "- monotonic_cst (array-like of int, shape=(n_features,), default=None):\n",
    "\n",
    "Description: Monotonicity constraints for each feature.\n",
    "1: Monotonically increasing.\n",
    "-1: Monotonically decreasing.\n",
    "0: No constraint.\n",
    "Limitations: Not supported for multioutput regressions or datasets with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c90d6e-cdb0-42af-b783-b8486a4a917f",
   "metadata": {},
   "source": [
    "### Attributes:\n",
    "- estimators_: List of individual DecisionTreeRegressor objects.\n",
    "- feature_importances_: Importance of each feature based on impurity reduction.\n",
    "- n_features_in_: Number of features seen during fit.\n",
    "- oob_score_: Out-of-bag score if oob_score=True.\n",
    "- oob_prediction_: Predictions computed with out-of-bag estimates.\n",
    "- classes_: Not applicable for regressors.\n",
    "- n_outputs_: Number of outputs when fit is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b18e174-3b4d-4cd9-a55b-829ece5d8674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gyanb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([33.22904762, 24.41816667,  9.01030303, 20.7577729 , 13.632     ,\n",
       "       23.34861111, 19.27886553, 16.23      , 20.76410654, 27.48033333,\n",
       "       16.96571429, 21.31838655, 21.9598456 , 18.27983991, 21.01716492,\n",
       "       23.445     ,  9.62      , 19.49544643, 16.50374338,  8.20363636,\n",
       "       37.06958333, 21.29366667, 20.29895691, 16.27      , 22.56667355,\n",
       "       20.0849584 , 20.95513593, 22.67000689, 27.93266667, 18.25666667,\n",
       "       21.06422335, 15.022     , 19.63876523, 13.625     , 23.79125   ,\n",
       "       19.1       , 20.95613165, 21.9706193 ,  9.4475    ,  8.31030303,\n",
       "        7.79030303, 23.74582284, 47.18      , 27.76      , 16.44193317,\n",
       "       25.98      , 26.782     , 48.83      , 15.19      , 22.38664953,\n",
       "       19.20149846, 33.454     , 23.86155556, 23.87316667, 39.61      ,\n",
       "       30.52      ,  9.53921212, 32.3       , 26.27      , 20.82133425,\n",
       "       39.27      , 15.41583333, 26.18818182, 43.035     ,  9.38254545,\n",
       "       18.97180976, 36.36      , 20.09538819, 49.38      , 41.07      ,\n",
       "       17.27137681, 23.81742063, 13.17      , 20.18166667, 23.87316667,\n",
       "       14.96194444, 37.92      , 22.205     , 16.36333333, 14.7852381 ,\n",
       "       45.4       , 43.46      , 24.96042857, 16.22238095, 25.45278617,\n",
       "       11.00454545, 27.01083333, 35.875     , 19.9809208 , 21.78264208,\n",
       "       16.792     , 15.91292443, 22.04114565, 17.896875  , 15.40333333,\n",
       "       44.91      , 16.322     , 23.707     , 29.43166667, 27.68283333,\n",
       "       18.587666  , 22.07039831])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor(n_estimators=10,max_depth=10,min_samples_split=2)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions=clf.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "697e7354-34bc-4e53-a2c1-165603823940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.005985960774359"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f567c6b3-a3c7-4070-87bd-ebf6890cf1f9",
   "metadata": {},
   "source": [
    "### Important Notes:\n",
    "- Feature Importance:<br>\n",
    "Calculated based on the total decrease in impurity brought by each feature.\n",
    "May be misleading for high-cardinality features (many unique values).\n",
    "Consider using sklearn.inspection.permutation_importance for more reliable feature importance.\n",
    "\n",
    "- Out-of-Bag Error:<br>\n",
    "Provides an internal estimate of model performance without needing a separate validation set.\n",
    "Useful for assessing the generalization error.\n",
    "\n",
    "- Pruning and Complexity:<br>\n",
    "Default parameters lead to fully grown and unpruned trees, which can be large.\n",
    "Control tree size and memory consumption by adjusting parameters like max_depth, min_samples_split, min_samples_leaf, and ccp_alpha.\n",
    "\n",
    "- Deterministic Behavior:<br>\n",
    "To ensure reproducible results, especially when max_features=n_features and bootstrap=False, set random_state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb52c76-3ad5-441d-aa8f-2c17439ecfec",
   "metadata": {},
   "source": [
    "### Example : python\n",
    "- from sklearn.ensemble import RandomForestRegressor<br>\n",
    "from sklearn.datasets import make_regression<br>\n",
    "from sklearn.model_selection import train_test_split<br>\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "-  Generate a sample regression dataset<br>\n",
    "X, y = make_regression(n_samples=1000, n_features=4, n_informative=2, noise=0.1, random_state=0)\n",
    "\n",
    "-  Split into training and testing sets<br>\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "- Instantiate and train the regressor<br>\n",
    "regr = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)<br>\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "- Make predictions<br>\n",
    "predictions = regr.predict(X_test)\n",
    "\n",
    "- Evaluate the model<br>\n",
    "mse = mean_squared_error(y_test, predictions)<br>\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54f291-c421-4a76-aba5-68ca0fcc7467",
   "metadata": {},
   "source": [
    "### Use Cases:\n",
    "- Regression Tasks:<br>\n",
    "Predicting continuous outcomes such as house prices, stock prices, or any other real-valued target.\n",
    "- Handling High-Dimensional Data:<br>\n",
    "Effective in scenarios with a large number of features.\n",
    "- Robustness to Overfitting:<br>\n",
    "Ensemble of trees reduces the risk of overfitting compared to individual decision trees.\n",
    "- Feature Importance Assessment:<br>\n",
    "Identifying which features contribute most to the prediction task.\n",
    "\n",
    "### When to Use:\n",
    "- When you need a model that balances bias and variance effectively.\n",
    "- For datasets where the relationship between features and target is complex and non-linear.\n",
    "- When interpretability of feature importance is valuable.\n",
    "\n",
    "### Alternatives:\n",
    "- DecisionTreeRegressor: Simpler model but more prone to overfitting.\n",
    "- ExtraTreesRegressor: Similar to Random Forest but uses a different method for splitting nodes, often resulting in faster training.\n",
    "- HistGradientBoostingRegressor: Suitable for very large datasets with high performance and speed.\n",
    "\n",
    "\n",
    "The RandomForestRegressor is a highly effective tool for regression problems, offering strong predictive performance and the ability to handle a variety of data types and complexities. Proper tuning of its parameters can lead to robust models that generalize well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7f415-8796-4b92-9c4c-23c8518c89ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
