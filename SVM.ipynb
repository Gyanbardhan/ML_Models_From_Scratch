{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf4193a-310f-4872-8bf5-a08dd1272c44",
   "metadata": {},
   "source": [
    "# SVC (Support Vector Classification) \n",
    "SVC (Support Vector Classification) class from the sklearn.svm module.<br>\n",
    "class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
    "### Overview of SVC\n",
    "Purpose: SVC is used for classification tasks. It implements the Support Vector Machine algorithm for binary and multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d4368b-22d5-4930-90c4-c02ed4e682a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc443ab5-6752-4d10-ac45-dfc9580dd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self,learning_rate=0.0001,lambda_param=0.01,n_iters=1000):\n",
    "        self.lr=learning_rate\n",
    "        self.lambda_param=lambda_param\n",
    "        self.n_iters=n_iters\n",
    "        self.weights=None\n",
    "        self.bias=None\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        y=np.where(y<=0,-1,1)\n",
    "        n_samples,n_features=X.shape\n",
    "        self.weights=np.zeros(n_features)\n",
    "        self.bias=0\n",
    "        for i in range(self.n_iters):\n",
    "            for idx,x_i in enumerate(X):\n",
    "                condition=y[idx]*(np.dot(x_i,self.weights)-self.bias)>=1\n",
    "                if condition:\n",
    "                    self.weights-=self.lr*(2*self.lambda_param*self.weights)\n",
    "                else:\n",
    "                    self.weights-=self.lr*(2*self.lambda_param*self.weights-np.dot(x_i,y[idx]))\n",
    "                    self.bias-=self.lr*y[idx]\n",
    "\n",
    "    def predict(self,X):\n",
    "        y_pred=np.dot(X,self.weights)-self.bias\n",
    "        return np.where(y_pred<=0,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60775e4a-90f4-42bb-a83d-fc79a48720b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X,y=datasets.make_blobs(n_samples=1000,n_features=5,centers=2,cluster_std=1.05,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d67fa9-1b3e-41f1-804d-5d04490fbd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.where(y==0,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1085f48-4c74-437a-b120-321fe99f02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127d044c-b629-490b-a864-26de1724a528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1, -1, -1, -1,  1,  1,\n",
       "        1, -1, -1, -1,  1, -1,  1,  1, -1,  1, -1, -1, -1,  1,  1,  1, -1,\n",
       "        1, -1, -1, -1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1, -1, -1,  1,\n",
       "       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, -1,  1, -1,  1,  1, -1,  1,  1,  1, -1,  1, -1,\n",
       "        1,  1, -1, -1, -1, -1,  1,  1,  1,  1, -1,  1, -1, -1,  1, -1, -1,\n",
       "        1,  1, -1, -1,  1,  1, -1,  1, -1, -1,  1, -1,  1,  1,  1, -1, -1,\n",
       "        1,  1,  1, -1,  1, -1,  1,  1, -1,  1,  1,  1,  1, -1, -1,  1, -1,\n",
       "       -1,  1, -1, -1,  1,  1, -1, -1,  1,  1,  1, -1, -1, -1,  1, -1, -1,\n",
       "       -1,  1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,  1, -1,  1,  1,\n",
       "        1, -1, -1,  1,  1, -1, -1, -1,  1, -1,  1,  1, -1,  1, -1,  1, -1,\n",
       "        1, -1,  1, -1,  1,  1,  1,  1, -1,  1,  1, -1, -1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=SVM()\n",
    "clf.fit(X_train,y_train)\n",
    "predictions=clf.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db098457-0042-43ae-aa76-e2485a59484c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1, -1, -1, -1,  1,  1,\n",
       "        1, -1, -1, -1,  1, -1,  1,  1, -1,  1, -1, -1, -1,  1,  1,  1, -1,\n",
       "        1, -1, -1, -1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1, -1, -1,  1,\n",
       "       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, -1,  1, -1,  1,  1, -1,  1,  1,  1, -1,  1, -1,\n",
       "        1,  1, -1, -1, -1, -1,  1,  1,  1,  1, -1,  1, -1, -1,  1, -1, -1,\n",
       "        1,  1, -1, -1,  1,  1, -1,  1, -1, -1,  1, -1,  1,  1,  1, -1, -1,\n",
       "        1,  1,  1, -1,  1, -1,  1,  1, -1,  1,  1,  1,  1, -1, -1,  1, -1,\n",
       "       -1,  1, -1, -1,  1,  1, -1, -1,  1,  1,  1, -1, -1, -1,  1, -1, -1,\n",
       "       -1,  1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,  1, -1,  1,  1,\n",
       "        1, -1, -1,  1,  1, -1, -1, -1,  1, -1,  1,  1, -1,  1, -1,  1, -1,\n",
       "        1, -1,  1, -1,  1,  1,  1,  1, -1,  1,  1, -1, -1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ad33192-cf3f-4a79-85d9-93ab25efd3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(y_true,y_pred):\n",
    "    accuracy=np.sum(y_true==y_pred)/len(y_true)\n",
    "    return accuracy\n",
    "accuracy(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5730f54-5d51-4e87-a3bb-b5e436a3b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc07ea9-be7a-4a97-a061-2d08e6097edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"diabetes.csv\")\n",
    "X=df.drop(columns=[\"Outcome\"])\n",
    "y=df[\"Outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f0cb865-89a8-4720-9c06-6f62d2af0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1234)\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "x_test=np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de2ae7f-2cc3-43b7-9cbb-d0f5d145820b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1,  1, -1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1,  1, -1,\n",
       "       -1, -1, -1, -1,  1, -1,  1, -1, -1,  1,  1, -1, -1, -1,  1,  1, -1,\n",
       "        1,  1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1, -1,\n",
       "       -1,  1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,\n",
       "       -1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1, -1,  1,  1,  1, -1,\n",
       "       -1, -1,  1, -1, -1, -1,  1, -1, -1, -1, -1, -1,  1, -1,  1, -1,  1,\n",
       "       -1,  1, -1, -1,  1,  1,  1,  1, -1,  1, -1, -1, -1,  1, -1,  1, -1,\n",
       "       -1, -1, -1, -1,  1,  1,  1, -1, -1, -1, -1,  1, -1, -1,  1,  1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1,  1, -1, -1,  1,  1, -1, -1,  1, -1, -1,\n",
       "        1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=SVM()\n",
    "clf.fit(X_train,y_train)\n",
    "predictions=clf.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3fb75ff-1cec-42ea-a0fc-e559d7cb9444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  1,  1, -1, -1, -1, -1,  1, -1,  1, -1, -1, -1, -1, -1,  1,\n",
       "        1, -1,  1,  1,  1, -1,  1, -1, -1, -1,  1, -1, -1, -1, -1,  1,  1,\n",
       "        1, -1,  1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1, -1, -1,  1,\n",
       "       -1,  1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1, -1,  1,  1,  1,  1, -1,\n",
       "       -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,  1,  1,  1,\n",
       "       -1, -1, -1,  1,  1,  1, -1,  1, -1, -1,  1,  1, -1, -1,  1,  1, -1,\n",
       "       -1, -1,  1,  1, -1,  1,  1, -1, -1, -1, -1, -1, -1,  1,  1, -1, -1,\n",
       "       -1,  1, -1, -1,  1, -1, -1, -1,  1,  1,  1, -1, -1, -1,  1, -1,  1,\n",
       "       -1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=np.where(y_test<=0,-1,1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d232fe-6528-46fc-9634-9e1f996f5fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6688311688311688"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dd959f3-e029-41c1-b446-1848b7a6bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.where(y_train<=0,-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0796264-3946-4db4-99f8-78d58026cfc5",
   "metadata": {},
   "source": [
    "### Key Parameters\n",
    "- C: Regularization parameter. A smaller value specifies stronger regularization, helping to prevent overfitting.\n",
    "- kernel: Defines the type of kernel function to be used (e.g., linear, rbf, poly). The choice of kernel can significantly affect model performance.\n",
    "- gamma: Affects the decision boundary's curvature. Higher values lead to more complex models, while lower values can lead to underfitting.\n",
    "- class_weight: Useful for handling imbalanced datasets. It adjusts the weight of each class based on their frequency.\n",
    "- probability: When set to True, enables probability estimates, but adds computational overhead during training.\n",
    "\n",
    "### Attributes\n",
    "- support_vectors_: Provides the support vectors, which are critical to the decision boundary.\n",
    "- n_support_: Returns the number of support vectors for each class.\n",
    "- coef_: Weights assigned to features if the kernel is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36352dc8-37fc-4728-945b-28c0dc4b0d44",
   "metadata": {},
   "source": [
    "### Methods\n",
    "- fit(X, y): Trains the model using the provided training data.\n",
    "- predict(X): Classifies the input data.\n",
    "- score(X, y): Returns the mean accuracy of the classifier on the test data.\n",
    "- predict_proba(X): Provides probabilities of class membership if probability estimates are enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aebe618d-5866-424c-b8e6-507de3025213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gyanb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1, -1, -1, -1, -1, -1,  1, -1,  1, -1, -1, -1, -1, -1,  1,\n",
       "       -1, -1, -1,  1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1, -1,\n",
       "        1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
       "        1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,  1, -1,  1,\n",
       "       -1,  1, -1, -1,  1,  1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1, -1,\n",
       "       -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1,\n",
       "        1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf=SVC()\n",
    "clf.fit(X_train,y_train)\n",
    "predictions=clf.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "257396f0-a21f-446f-837d-9b81bb4f54c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7467532467532467"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085a127-5b6b-4881-ba69-18192eaed2e8",
   "metadata": {},
   "source": [
    "### Example\n",
    "Here's a simple example of using SVC with a pipeline:\n",
    "\n",
    "- import numpy as np<br>\n",
    "from sklearn.pipeline import make_pipeline<br>\n",
    "from sklearn.preprocessing import StandardScaler<br>\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "- Sample data<br>\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])<br>\n",
    "y = np.array([1, 1, 2, 2])\n",
    "\n",
    "- Create a pipeline with standard scaling and SVC<br>\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='scale<br>\n",
    "clf.fit(X, y)\n",
    "\n",
    "- Make a prediction<br>\n",
    "print(clf.predict([[-0.8, -1]]))  # Output might be [1]\n",
    "\n",
    "### Considerations\n",
    "- Performance: For large datasets, consider alternatives like LinearSVC or SGDClassifier, as SVC can become computationally intensive.\n",
    "- Hyperparameter Tuning: Tuning parameters like C, gamma, and kernel choice can significantly improve performance. Use techniques like cross-validation to find optimal values.\n",
    "- Interpretability: While SVMs can provide a strong classification performance, the interpretability of the decision boundaries may be complex, especially with non-linear kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5796b6fe-e710-4536-a222-f593f7445ac1",
   "metadata": {},
   "source": [
    "### Kernel Function in Support Vector Machine\n",
    "In Support Vector Machines (SVMs), there are several types of kernel functions that can be used to map the input data into a higher-dimensional feature space. The choice of kernel function depends on the specific problem and the characteristics of the data.\n",
    "\n",
    "- Linear Kernel<br>\n",
    "A linear kernel is a type of kernel function used in machine learning, including in SVMs (Support Vector Machines). It is the simplest and most commonly used kernel function, and it defines the dot product between the input vectors in the original feature space.<br>\n",
    "The linear kernel can be defined as:<br>\n",
    "K(x, y) = x .y  <br>\n",
    "Where x and y are the input feature vectors. The dot product of the input vectors is a measure of their similarity or distance in the original feature space.<br>\n",
    "When using a linear kernel in an SVM, the decision boundary is a linear hyperplane that separates the different classes in the feature space. This linear boundary can be useful when the data is already separable by a linear decision boundary or when dealing with high-dimensional data, where the use of more complex kernel functions may lead to overfitting.\n",
    "\n",
    "- Polynomial Kernel\n",
    "A particular kind of kernel function utilised in machine learning, such as in SVMs, is a polynomial kernel (Support Vector Machines). It is a nonlinear kernel function that employs polynomial functions to transfer the input data into a higher-dimensional feature space.<br>\n",
    "One definition of the polynomial kernel is:<br>\n",
    "K(x, y) = (x. y + c)d <br>\n",
    "Where x and y are the input feature vectors, c is a constant term, and d is the degree of the polynomial.<br>\n",
    "The constant term is added to, and the dot product of the input vectors elevated to the degree of the polynomial.<br>\n",
    "The decision boundary of an SVM with a polynomial kernel might capture more intricate correlations between the input characteristics because it is a nonlinear hyperplane.<br>\n",
    "The degree of nonlinearity in the decision boundary is determined by the degree of the polynomial.<br>\n",
    "The polynomial kernel has the benefit of being able to detect both linear and nonlinear correlations in the data. It can be difficult to select the proper degree of the polynomial, though, as a larger degree can result in overfitting while a lower degree cannot adequately represent the underlying relationships in the data.<br>\n",
    "In general, the polynomial kernel is an effective tool for converting the input data into a higher-dimensional feature space in order to capture nonlinear correlations between the input characteristics.<br>\n",
    "\n",
    "- Gaussian (RBF) Kernel<br>\n",
    "The Gaussian kernel, also known as the radial basis function (RBF) kernel, is a popular kernel function used in machine learning, particularly in SVMs (Support Vector Machines). It is a nonlinear kernel function that maps the input data into a higher-dimensional feature space using a Gaussian function.<br>\n",
    "The Gaussian kernel can be defined as:<br>\n",
    "K(x, y) = exp(-gamma * ||x - y||^2)  <br>\n",
    "Where x and y are the input feature vectors, gamma is a parameter that controls the width of the Gaussian function, and ||x - y||^2 is the squared Euclidean distance between the input vectors.<br>\n",
    "When using a Gaussian kernel in an SVM, the decision boundary is a nonlinear hyper plane that can capture complex nonlinear relationships between the input features. The width of the Gaussian function, controlled by the gamma parameter, determines the degree of nonlinearity in the decision boundary.<br>\n",
    "One advantage of the Gaussian kernel is its ability to capture complex relationships in the data without the need for explicit feature engineering. However, the choice of the gamma parameter can be challenging, as a smaller value may result in under fitting, while a larger value may result in over fitting.<br>\n",
    "\n",
    "- Laplace Kernel<br>\n",
    "The Laplacian kernel, also known as the Laplace kernel or the exponential kernel, is a type of kernel function used in machine learning, including in SVMs (Support Vector Machines). It is a non-parametric kernel that can be used to measure the similarity or distance between two input feature vectors.<br>\n",
    "The Laplacian kernel can be defined as:<br>\n",
    "K(x, y) = exp(-gamma * ||x - y||)  <br>\n",
    "Where x and y are the input feature vectors, gamma is a parameter that controls the width of the Laplacian function, and ||x - y|| is the L1 norm or Manhattan distance between the input vectors.<br>\n",
    "When using a Laplacian kernel in an SVM, the decision boundary is a nonlinear hyperplane that can capture complex relationships between the input features. The width of the Laplacian function, controlled by the gamma parameter, determines the degree of nonlinearity in the decision boundary.<br>\n",
    "One advantage of the Laplacian kernel is its robustness to outliers, as it places less weight on large distances between the input vectors than the Gaussian kernel. However, like the Gaussian kernel, choosing the correct value of the gamma parameter can be challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9f61f-0b04-40c7-bb65-7cc11f4a57d3",
   "metadata": {},
   "source": [
    "### Advantages of SVM:\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "### Disadvantages of SVM:\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3be0f-8bab-4994-8e28-ce8c1a6397c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
